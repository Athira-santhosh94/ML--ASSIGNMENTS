{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330c2485-7625-4093-a5ba-40f771e1fe37",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification - Supervised Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8f05a-f31b-43b8-8eac-d5c2cd085f26",
   "metadata": {},
   "source": [
    "## Step 1: Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae06f76-20c3-4102-ac36-a246ac4390f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85183c07-2438-4cac-8b63-007a1df88ef7",
   "metadata": {},
   "source": [
    "## Step 2: Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ab2a1b-97af-4771-93c2-7ba9f6aea7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c3471d-2292-4326-8d25-8b605e3e1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in dataset:\", X.isnull().sum().sum()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a300724-4120-4b71-8dd6-98d442b6f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and preprocessed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Splitting dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(\"Dataset loaded and preprocessed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a2ccd-67b7-4860-98c1-78b814a708d0",
   "metadata": {},
   "source": [
    "### Preprocessing Steps and Justification\n",
    "\n",
    "1. **Loading the Dataset**:\n",
    "   - The breast cancer dataset was loaded from `sklearn.datasets`, which includes 569 samples with 30 numeric features used to predict whether a tumor is malignant or benign.\n",
    "\n",
    "2. **Checking for Missing Values**:\n",
    "   - `X.isnull().sum().sum()` was used to check for any missing values. The result was 0, which means the dataset is clean and does not require imputation.\n",
    "\n",
    "3. **Feature Scaling**:\n",
    "   - `StandardScaler` was applied to scale the features so they have a mean of 0 and a standard deviation of 1.\n",
    "   - **Why?** Many machine learning algorithms (like SVM, k-NN, and logistic regression) are sensitive to the scale of the input features. Scaling ensures that all features contribute equally to distance or optimization calculations.\n",
    "\n",
    "4. **Train-Test Split**:\n",
    "   - The dataset was split into 80% training and 20% testing data using `train_test_split` with a fixed random state for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaab1c-d28b-439c-8ba9-116fab61cd0b",
   "metadata": {},
   "source": [
    "## Step 3: Implementing Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb66ca-c088-4778-b17b-071177153c85",
   "metadata": {},
   "source": [
    " **1.Logistic Regression**:\n",
    "   - **How it works**: Logistic Regression models the probability that a given input belongs to a certain class using a logistic function (sigmoid).\n",
    "   - **Suitability**: It is suitable for binary classification problems like this one (malignant vs. benign). It is simple, interpretable, and often serves as a strong baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eb83ce7-bc37-4514-b1b2-b654438f2f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Logistic Regression:\n",
      "Logistic Regression models the probability of default class using a sigmoid function.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "results['Logistic Regression'] = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"\\n1. Logistic Regression:\")\n",
    "print(\"Logistic Regression models the probability of default class using a sigmoid function.\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3f119-b3c0-4d21-b4ae-7b47300fdc13",
   "metadata": {},
   "source": [
    "**2.Decision Tree Classifier**:\n",
    "   - **How it works**: This algorithm splits the data recursively based on feature thresholds, forming a tree where each internal node represents a decision based on a feature.\n",
    "   - **Suitability**: It can handle non-linear relationships and does not require feature scaling. It is easy to visualize and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4b357be-c371-48bf-aa3c-6148c4f33492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Decision Tree Classifier:\n",
      "Decision Trees split the data recursively based on feature thresholds to make predictions.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred_dt = dtree.predict(X_test)\n",
    "results['Decision Tree'] = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"\\n2. Decision Tree Classifier:\")\n",
    "print(\"Decision Trees split the data recursively based on feature thresholds to make predictions.\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9090fb-351e-418c-bdc6-c13fd158c895",
   "metadata": {},
   "source": [
    "**3.Random Forest Classifier**:\n",
    "   - **How it works**: An ensemble method that builds multiple decision trees and combines their outputs (usually by majority voting) to improve accuracy and reduce overfitting.\n",
    "   - **Suitability**: Random Forests are robust, handle noise well, and are generally one of the most accurate classification methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1027a039-941b-4922-bfd3-df92026174eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Random Forest Classifier:\n",
      "Random Forest uses multiple decision trees and aggregates their output for better performance.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rforest = RandomForestClassifier(random_state=42)\n",
    "rforest.fit(X_train, y_train)\n",
    "y_pred_rf = rforest.predict(X_test)\n",
    "results['Random Forest'] = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\n3. Random Forest Classifier:\")\n",
    "print(\"Random Forest uses multiple decision trees and aggregates their output for better performance.\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b6967-831b-455c-ba9a-d2c4e83b9059",
   "metadata": {},
   "source": [
    "**4.Support Vector Machine (SVM)**:\n",
    "   - **How it works**: SVM finds the hyperplane that best separates the classes by maximizing the margin between them.\n",
    "   - **Suitability**: SVM is effective in high-dimensional spaces, which is useful here since the dataset has 30 features. It performs well with clear margin separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c56b279c-0f56-44e3-af4d-75df71c9fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Support Vector Machine (SVM):\n",
      "SVM finds the optimal hyperplane that separates classes with maximum margin.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "results['SVM'] = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "print(\"\\n4. Support Vector Machine (SVM):\")\n",
    "print(\"SVM finds the optimal hyperplane that separates classes with maximum margin.\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036b598-5742-45c3-844d-c8d32d4078d5",
   "metadata": {},
   "source": [
    "**5.k-Nearest Neighbors (k-NN)**:\n",
    "   - **How it works**: Classifies a sample based on the most common class among its k nearest neighbors in the feature space.\n",
    "   - **Suitability**: Simple and intuitive. However, it can be sensitive to the choice of `k` and requires feature scaling due to distance-based calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed39fe9a-a1c1-4a88-a029-221f64d4950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. k-Nearest Neighbors (k-NN):\n",
      "k-NN classifies a data point based on majority class among its k nearest neighbors.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "results['k-NN'] = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(\"\\n5. k-Nearest Neighbors (k-NN):\")\n",
    "print(\"k-NN classifies a data point based on majority class among its k nearest neighbors.\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb5c1b-c83e-42f5-89de-49ee91fc13d7",
   "metadata": {},
   "source": [
    "## Step 4: Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4806df21-573b-47dc-8c96-3d91d318b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison (Accuracy Scores):\n",
      "Logistic Regression: 0.9737\n",
      "Decision Tree: 0.9474\n",
      "Random Forest: 0.9649\n",
      "SVM: 0.9737\n",
      "k-NN: 0.9474\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Comparison (Accuracy Scores):\")\n",
    "for model, acc in results.items():\n",
    "    print(f\"{model}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "215a8011-7122-4e21-b380-9515f28338d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model: Logistic Regression with accuracy 0.9737\n",
      "Worst Performing Model: Decision Tree with accuracy 0.9474\n"
     ]
    }
   ],
   "source": [
    "# Identifying best and worst\n",
    "best_model = max(results, key=results.get)\n",
    "worst_model = min(results, key=results.get)\n",
    "\n",
    "print(f\"\\nBest Performing Model: {best_model} with accuracy {results[best_model]:.4f}\")\n",
    "print(f\"Worst Performing Model: {worst_model} with accuracy {results[worst_model]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25e8f7-6b6c-4d69-9011-24d45f64563e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
