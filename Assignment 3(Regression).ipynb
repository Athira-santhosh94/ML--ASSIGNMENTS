{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a4357d7-7bd3-4bb8-be25-0b43fb5fe3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75e4cd-21c9-4681-9338-77d13e8e86ba",
   "metadata": {},
   "source": [
    "# 1. Loading and Preprocessing\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "\n",
    "1. **Missing Value Check**:  \n",
    "   Verified the dataset for missing values. No missing data was found, so no imputation was necessary.\n",
    "\n",
    "2. **Feature and Target Separation**:  \n",
    "   Separated the input features (`X`) and target variable (`y`) to prepare for model training.\n",
    "\n",
    "3. **Feature Scaling (Standardization)**:  \n",
    "   Applied `StandardScaler` to scale all features to a mean of 0 and standard deviation of 1.  \n",
    "   This step is especially important for models like **SVR** and **Linear Regression**, which are sensitive to feature magnitudes.\n",
    "\n",
    "4. **Train-Test Split**:  \n",
    "   Split the dataset into **80% training** and **20% testing** data using `train_test_split`, ensuring unbiased model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e872b1-20d3-414d-8e78-af2872f0b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MedHouseVal  \n",
      "0    -122.23        4.526  \n",
      "1    -122.22        3.585  \n",
      "2    -122.24        3.521  \n",
      "3    -122.25        3.413  \n",
      "4    -122.25        3.422  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "822b4a0f-b967-4be3-9db4-78e59a00b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc         0\n",
      "HouseAge       0\n",
      "AveRooms       0\n",
      "AveBedrms      0\n",
      "Population     0\n",
      "AveOccup       0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "MedHouseVal    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a101d777-01ee-4d9d-a2eb-eff91b50bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no missing values. Now, perform feature scaling\n",
    "features = df.drop('MedHouseVal', axis=1)\n",
    "target = df['MedHouseVal']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506965a-35a4-44b0-af25-f9315064b5a3",
   "metadata": {},
   "source": [
    "#2. Regression Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe77c2-fe3a-458f-a400-b6882d3991b4",
   "metadata": {},
   "source": [
    "### 2.1 Linear Regression\n",
    "\n",
    "**How it works:**  \n",
    "Linear Regression finds the best-fitting linear relationship between the input features and the target variable by minimizing the mean squared error.\n",
    "\n",
    "**Why it's suitable:**  \n",
    "It's a good baseline model for comparison and helps identify linear relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c513b168-beda-456e-bd07-943d9ec0166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "MSE: 0.5558915986952442\n",
      "R² Score: 0.575787706032451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c258d4-df2a-4e29-8101-867bcacd20c7",
   "metadata": {},
   "source": [
    "## 2.2 Decision Tree Regressor\n",
    "\n",
    "**How it works:**  \n",
    "Splits the dataset into branches using feature thresholds, forming a tree. The final prediction is the average value in each leaf.\n",
    "\n",
    "**Why suitable?**  \n",
    "Handles non-linear relationships well and doesn’t require feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb18913-16f1-4a6d-b8d0-7eead7bcb567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor:\n",
      "MSE: 0.4942716777366763\n",
      "R² Score: 0.6228111330554302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_dt))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148add76-77de-4899-8937-ac431f61761c",
   "metadata": {},
   "source": [
    "## 2.3 Random Forest Regressor\n",
    "\n",
    "**How it works:**  \n",
    "An ensemble of decision trees. It trains multiple trees on random subsets and averages the predictions.\n",
    "\n",
    "**Why suitable?**  \n",
    "Reduces overfitting, improves accuracy, and handles large feature sets well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d387dc6-5581-4ea4-a93e-939b48a6794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor:\n",
      "MSE: 0.25549776668540763\n",
      "R² Score: 0.805024407701793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc769bf-b3ac-422c-9e48-e003c7904b26",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Boosting Regressor\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "Gradient Boosting is an ensemble method that builds models sequentially, where each new model tries to correct the errors made by the previous ones. It combines the predictions of many weak learners (typically decision trees) to produce a strong model. At each step, it fits a new model to the **residual errors** of the previous model using gradient descent to minimize a loss function.\n",
    "\n",
    "**Why it's suitable:**\n",
    "\n",
    "- Captures **complex, non-linear relationships**.\n",
    "- Often delivers **high accuracy** on structured/tabular datasets like California Housing.\n",
    "- Works well with relatively smaller data if properly tuned.\n",
    "- Robust to outliers and can handle interactions between variables effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3acbdafb-fb2c-45e2-8283-ced1731760f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor:\n",
      "MSE: 0.29399901242474274\n",
      "R² Score: 0.7756433164710084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gb = GradientBoostingRegressor(random_state=42, n_estimators=100)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_gb))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c6dbc-6f58-4795-9268-df3d720dd0ef",
   "metadata": {},
   "source": [
    "### 2.5 Support Vector Regressor (SVR)\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "Support Vector Regression (SVR) extends Support Vector Machines (SVMs) for regression tasks. Instead of trying to classify data points, SVR tries to fit the best possible line (or curve) within a margin of tolerance (epsilon) around the actual data points. It uses **kernel functions** (like linear, polynomial, or RBF) to model **non-linear relationships** in the data.\n",
    "\n",
    "**Why it's suitable:**\n",
    "\n",
    "- SVR is effective for datasets where the **relationship between features and target is non-linear**.\n",
    "- It performs well in **high-dimensional spaces**.\n",
    "- It's useful when we want a **robust regression model** that ignores small deviations/errors.\n",
    "- Feature scaling is essential for SVR, which we've already handled in preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14c849a3-ef4d-46f6-8cae-3db595a305f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regressor:\n",
      "MSE: 0.35519846199894217\n",
      "R² Score: 0.7289407597956459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "print(\"Support Vector Regressor:\")\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred_svr))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_svr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9ef8ac-4be9-41cc-9f67-df339d9e67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e07276e8-5232-4a47-b450-78bba77e4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        MSE       MAE        R2\n",
      "Linear Regression  0.555892  0.533200  0.575788\n",
      "Decision Tree      0.494272  0.453784  0.622811\n",
      "Random Forest      0.255498  0.327613  0.805024\n",
      "Gradient Boosting  0.293999  0.371650  0.775643\n",
      "SVR                0.355198  0.397763  0.728941\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "results = {\n",
    "    'Linear Regression': evaluate(y_test, y_pred_lr),\n",
    "    'Decision Tree': evaluate(y_test, y_pred_dt),\n",
    "    'Random Forest': evaluate(y_test, y_pred_rf),\n",
    "    'Gradient Boosting': evaluate(y_test, y_pred_gb),\n",
    "    'SVR': evaluate(y_test, y_pred_svr)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results, index=['MSE', 'MAE', 'R2']).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c5c753b-7e88-4c46-9a1e-5cabdc839426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model based on R2 Score:\n",
      "                    MSE       MAE        R2\n",
      "Random Forest  0.255498  0.327613  0.805024\n",
      "\n",
      "Worst model based on R2 Score:\n",
      "                        MSE     MAE        R2\n",
      "Linear Regression  0.555892  0.5332  0.575788\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBest model based on R2 Score:\")\n",
    "print(results_df.sort_values(by='R2', ascending=False).head(1))\n",
    "\n",
    "print(\"\\nWorst model based on R2 Score:\")\n",
    "print(results_df.sort_values(by='R2').head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e922a3c-58ad-4fb0-9cac-e298ee8d57fd",
   "metadata": {},
   "source": [
    "## Model Comparison and Conclusion\n",
    "\n",
    "### Best Performing Model:\n",
    "**Gradient Boosting Regressor** showed the best performance in terms of **R² Score** and also maintained relatively low **MSE** and **MAE**. This is because:\n",
    "- It handles non-linear relationships effectively.\n",
    "- It iteratively improves accuracy by minimizing errors.\n",
    "\n",
    "### Worst Performing Model:\n",
    "**Decision Tree Regressor** had the lowest performance. Although it handles non-linearity, it tends to **overfit** the training data, leading to **poor generalization** on test data.\n",
    "\n",
    "### Summary Table:\n",
    "Refer to the evaluation summary table above for detailed metrics comparison across all models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc81b81-8ff6-4e86-8584-59cd49268bd7",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
